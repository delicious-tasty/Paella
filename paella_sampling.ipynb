{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea0ec1-436f-4fca-99fa-5122a73b52d7",
   "metadata": {
    "id": "b3ea0ec1-436f-4fca-99fa-5122a73b52d7",
    "outputId": "b90ef2e0-156a-458d-f495-8a0fda0253e5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install kornia lpips einops rudalle open_clip_torch pytorch_lightning webdataset timm git+https://github.com/pabloppp/pytorch-tools git+https://github.com/openai/CLIP.git -U\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c89a6-4648-46e2-90c7-cf7c1b0dd861",
   "metadata": {
    "id": "788a2a72",
    "outputId": "702cbb5b-8c71-4b33-e86e-f84b718b7a1b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from modules import DenoiseUNet\n",
    "import open_clip\n",
    "from open_clip import tokenizer\n",
    "from rudalle import get_vae\n",
    "from einops import rearrange\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6640155",
   "metadata": {
    "id": "d6640155",
    "outputId": "99d4752b-3a85-49da-cbf3-72637160e2b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def showmask(mask):\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(torch.cat([\n",
    "        torch.cat([i for i in mask[0:1].cpu()], dim=-1),\n",
    "    ], dim=-2).cpu())\n",
    "    plt.show()\n",
    "\n",
    "def showimages(imgs, **kwargs):\n",
    "    plt.figure(figsize=(kwargs.get(\"width\", 32), kwargs.get(\"height\", 32)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(torch.cat([\n",
    "        torch.cat([i for i in imgs], dim=-1),\n",
    "    ], dim=-2).permute(1, 2, 0).cpu())\n",
    "    plt.show()\n",
    "    \n",
    "def saveimages(imgs, name, **kwargs):\n",
    "    name = name.replace(\" \", \"_\").replace(\".\", \"\")\n",
    "    path = os.path.join(\"outputs\", name + \".jpg\")\n",
    "    while os.path.exists(path):\n",
    "        base, ext = path.split(\".\")\n",
    "        num = base.split(\"_\")[-1]\n",
    "        if num.isdigit():\n",
    "            num = int(num) + 1\n",
    "            base = \"_\".join(base.split(\"_\")[:-1])\n",
    "        else:\n",
    "            num = 0\n",
    "        path = base + \"_\" + str(num) + \".\" + ext\n",
    "    torchvision.utils.save_image(imgs, path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e1f26-b4ca-4e11-947d-be80196d440f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log(t, eps=1e-20):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature=1., dim=-1):\n",
    "    return ((t / max(temperature, 1e-10)) + gumbel_noise(t)).argmax(dim=dim)\n",
    "\n",
    "def sample(model, c, x=None, mask=None, T=12, size=(32, 32), starting_t=0, temp_range=[1.0, 1.0], typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=-1, renoise_steps=11, renoise_mode='start'):\n",
    "    with torch.inference_mode():\n",
    "        r_range = torch.linspace(0, 1, T+1)[:-1][:, None].expand(-1, c.size(0)).to(c.device)\n",
    "        temperatures = torch.linspace(temp_range[0], temp_range[1], T)\n",
    "        preds = []\n",
    "        if x is None:\n",
    "            x = torch.randint(0, model.num_labels, size=(c.size(0), *size), device=c.device)\n",
    "        elif mask is not None:\n",
    "            noise = torch.randint(0, model.num_labels, size=(c.size(0), *size), device=c.device)\n",
    "            x = noise * mask + (1-mask) * x\n",
    "        init_x = x.clone()\n",
    "        for i in range(starting_t, T):\n",
    "            if renoise_mode == 'prev':\n",
    "                prev_x = x.clone()\n",
    "            r, temp = r_range[i], temperatures[i]\n",
    "            logits = model(x, c, r)\n",
    "            if classifier_free_scale >= 0:\n",
    "                logits_uncond = model(x, torch.zeros_like(c), r)\n",
    "                logits = torch.lerp(logits_uncond, logits, classifier_free_scale)\n",
    "            x = logits\n",
    "            x_flat = x.permute(0, 2, 3, 1).reshape(-1, x.size(1))\n",
    "            if typical_filtering:\n",
    "                x_flat_norm = torch.nn.functional.log_softmax(x_flat, dim=-1)\n",
    "                x_flat_norm_p = torch.exp(x_flat_norm)\n",
    "                entropy = -(x_flat_norm * x_flat_norm_p).nansum(-1, keepdim=True)\n",
    "\n",
    "                c_flat_shifted = torch.abs((-x_flat_norm) - entropy)\n",
    "                c_flat_sorted, x_flat_indices = torch.sort(c_flat_shifted, descending=False)\n",
    "                x_flat_cumsum = x_flat.gather(-1, x_flat_indices).softmax(dim=-1).cumsum(dim=-1)\n",
    "\n",
    "                last_ind = (x_flat_cumsum < typical_mass).sum(dim=-1)\n",
    "                sorted_indices_to_remove = c_flat_sorted > c_flat_sorted.gather(1, last_ind.view(-1, 1))\n",
    "                if typical_min_tokens > 1:\n",
    "                    sorted_indices_to_remove[..., :typical_min_tokens] = 0\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, x_flat_indices, sorted_indices_to_remove)\n",
    "                x_flat = x_flat.masked_fill(indices_to_remove, -float(\"Inf\"))\n",
    "            # x_flat = torch.multinomial(x_flat.div(temp).softmax(-1), num_samples=1)[:, 0]\n",
    "            x_flat = gumbel_sample(x_flat, temperature=temp)\n",
    "            x = x_flat.view(x.size(0), *x.shape[2:])\n",
    "            if mask is not None:\n",
    "                x = x * mask + (1-mask) * init_x\n",
    "            if i < renoise_steps:\n",
    "                if renoise_mode == 'start':\n",
    "                    x, _ = model.add_noise(x, r_range[i+1], random_x=init_x)\n",
    "                elif renoise_mode == 'prev':\n",
    "                    x, _ = model.add_noise(x, r_range[i+1], random_x=prev_x)\n",
    "                else: # 'rand'\n",
    "                    x, _ = model.add_noise(x, r_range[i+1])\n",
    "            preds.append(x.detach())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c7c626",
   "metadata": {
    "id": "34c7c626",
    "outputId": "16108313-6b3d-4751-d84b-a92d7f8ebf68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vqmodel = get_vae().to(device)\n",
    "vqmodel.eval().requires_grad_(False)\n",
    "\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms('ViT-g-14', pretrained='laion2b_s12b_b42k')\n",
    "clip_model = clip_model.to(device).eval().requires_grad_(False)\n",
    "\n",
    "clip_preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    # torchvision.transforms.CenterCrop(256),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def encode(x):\n",
    "    return vqmodel.model.encode((2 * x - 1))[-1][-1]\n",
    "    \n",
    "def decode(img_seq, shape=(32,32)):\n",
    "        img_seq = img_seq.view(img_seq.shape[0], -1)\n",
    "        b, n = img_seq.shape\n",
    "        one_hot_indices = torch.nn.functional.one_hot(img_seq, num_classes=vqmodel.num_tokens).float()\n",
    "        z = (one_hot_indices @ vqmodel.model.quantize.embed.weight)\n",
    "        z = rearrange(z, 'b (h w) c -> b c h w', h=shape[0], w=shape[1])\n",
    "        img = vqmodel.model.decode(z)\n",
    "        img = (img.clamp(-1., 1.) + 1) * 0.5\n",
    "        return img\n",
    "    \n",
    "state_dict = torch.load(\"./models/f8_600000.pt\", map_location=device)\n",
    "# state_dict = torch.load(\"./models/f8_img_40000.pt\", map_location=device)\n",
    "model = DenoiseUNet(num_labels=8192).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval().requires_grad_()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a98f2-bf40-4059-86f6-c83dac8c15eb",
   "metadata": {},
   "source": [
    "# Text-Conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cd5d7-220a-438b-8229-823fa9e3fff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = \"text\"\n",
    "batch_size = 6\n",
    "text = \"highly detailed photograph of darth vader. artstation\"\n",
    "latent_shape = (32, 32)\n",
    "tokenized_text = tokenizer.tokenize([text] * batch_size).to(device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        clip_embeddings = clip_model.encode_text(tokenized_text)\n",
    "        s = time.time()\n",
    "        sampled = sample(model, clip_embeddings, T=12, size=latent_shape, starting_t=0, temp_range=[1.0, 1.0],\n",
    "           typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=5, renoise_steps=11,\n",
    "           renoise_mode=\"start\")\n",
    "        print(time.time() - s)\n",
    "    sampled = decode(sampled[-1], latent_shape)\n",
    "\n",
    "showimages(sampled)\n",
    "saveimages(sampled, mode + \"_\" + text, nrow=len(sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5cecc-4e30-465e-bc51-425ec76a7d06",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae32e2",
   "metadata": {
    "id": "35ae32e2",
    "outputId": "28283340-fe61-44c6-9d92-720d04cf56bf"
   },
   "outputs": [],
   "source": [
    "mode = \"interpolation\"\n",
    "text = \"surreal painting of a yellow tulip. artstation\"\n",
    "text2 = \"surreal painting of a red tulip. artstation\"\n",
    "text_encoded = tokenizer.tokenize([text]).to(device)\n",
    "text2_encoded = tokenizer.tokenize([text2]).to(device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        clip_embeddings = clip_model.encode_text(text_encoded).float()\n",
    "        clip_embeddings2 = clip_model.encode_text(text2_encoded).float()\n",
    "\n",
    "        l = torch.linspace(0, 1, 10).to(device)\n",
    "        embeddings = []\n",
    "        for i in l:\n",
    "            lerp = torch.lerp(clip_embeddings, clip_embeddings2, i)\n",
    "            embeddings.append(lerp)\n",
    "        embeddings = torch.cat(embeddings)\n",
    "        \n",
    "        s = time.time()\n",
    "        sampled = sample(model, embeddings, T=12, size=(32, 32), starting_t=0, temp_range=[1.0, 1.0],\n",
    "               typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=4, renoise_steps=11)\n",
    "        print(time.time() - s)\n",
    "    sampled = decode(sampled[-1])\n",
    "showimages(sampled)\n",
    "saveimages(sampled, mode + \"_\" + text + \"_\" + text2, nrow=len(sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026fade5-0384-4521-992e-8cd66929d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"interpolation\"\n",
    "text = \"High quality front portrait photo of a tiger.\"\n",
    "text2 = \"High quality front portrait photo of a dog.\"\n",
    "text_encoded = tokenizer.tokenize([text]).to(device)\n",
    "text2_encoded = tokenizer.tokenize([text2]).to(device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        clip_embeddings = clip_model.encode_text(text_encoded).float()\n",
    "        clip_embeddings2 = clip_model.encode_text(text2_encoded).float()\n",
    "\n",
    "        l = torch.linspace(0, 1, 10).to(device)\n",
    "        s = time.time()\n",
    "        outputs = []\n",
    "        for i in l:\n",
    "            # lerp = torch.lerp(clip_embeddings, clip_embeddings2, i)\n",
    "            low, high = clip_embeddings, clip_embeddings2\n",
    "            low_norm = low/torch.norm(low, dim=1, keepdim=True)\n",
    "            high_norm = high/torch.norm(high, dim=1, keepdim=True)\n",
    "            omega = torch.acos((low_norm*high_norm).sum(1)).unsqueeze(1)\n",
    "            so = torch.sin(omega)\n",
    "            lerp = (torch.sin((1.0-i)*omega)/so)*low + (torch.sin(i*omega)/so) * high\n",
    "            with torch.random.fork_rng():\n",
    "                torch.random.manual_seed(32)\n",
    "                sampled = sample(model, lerp, T=20, size=(32, 32), starting_t=0, temp_range=[1.0, 1.0],\n",
    "                       typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=5, renoise_steps=11)\n",
    "                outputs.append(sampled[-1])\n",
    "        print(time.time() - s)\n",
    "    sampled = torch.cat(outputs)\n",
    "    sampled = decode(sampled)\n",
    "showimages(sampled)\n",
    "saveimages(sampled, mode + \"_\" + text + \"_\" + text2, nrow=len(sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561133a9",
   "metadata": {
    "id": "0bd51975"
   },
   "source": [
    "# Multi-Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9ac9f",
   "metadata": {
    "id": "83e9ac9f"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "latent_shape = (32, 32)\n",
    "text_a = \"a cute portrait of a dog\"\n",
    "text_b = \"a cute portrait of a cat\"\n",
    "mode = \"vertical\"\n",
    "# mode = \"horizontal\"\n",
    "text = tokenizer.tokenize([text_a, text_b] * batch_size).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        clip_embeddings = clip_model.encode_text(text).float()[:, :, None, None].expand(-1, -1, latent_shape[0], latent_shape[1])\n",
    "        if mode == 'vertical':\n",
    "            interp_mask = torch.linspace(0, 1, latent_shape[0], device=device)[None, None, :, None].expand(batch_size, 1, -1, latent_shape[1])\n",
    "        else: \n",
    "            interp_mask = torch.linspace(0, 1, latent_shape[1], device=device)[None, None, None, :].expand(batch_size, 1, latent_shape[0], -1)\n",
    "        # LERP\n",
    "        clip_embeddings = clip_embeddings[0::2] * (1-interp_mask) + clip_embeddings[1::2] * interp_mask\n",
    "        # # SLERP\n",
    "        # low, high = clip_embeddings[0::2], clip_embeddings[1::2]\n",
    "        # low_norm = low/torch.norm(low, dim=1, keepdim=True)\n",
    "        # high_norm = high/torch.norm(high, dim=1, keepdim=True)\n",
    "        # omega = torch.acos((low_norm*high_norm).sum(1)).unsqueeze(1)\n",
    "        # so = torch.sin(omega)\n",
    "        # clip_embeddings = (torch.sin((1.0-interp_mask)*omega)/so)*low + (torch.sin(interp_mask*omega)/so) * high\n",
    "    \n",
    "        sampled = sample(model, clip_embeddings, T=12, size=latent_shape, starting_t=0, temp_range=[1.0, 1.0],\n",
    "           typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=5, renoise_steps=11,\n",
    "           renoise_mode=\"start\")\n",
    "    sampled = decode(sampled[-1], latent_shape)\n",
    "\n",
    "showimages(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838e233-34fd-4637-b0df-476f4e66cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"multiconditioning\"\n",
    "batch_size = 4\n",
    "latent_shape = (32, 32)\n",
    "conditions = [\n",
    "    [\"High quality portrait of a dog.\", 16],\n",
    "    [\"High quality portrait of a wolf.\", 32],\n",
    "]\n",
    "clip_embedding = torch.zeros(batch_size, 1024, *latent_shape).to(device)\n",
    "last_pos = 0\n",
    "for text, pos in conditions:\n",
    "    tokenized_text = tokenizer.tokenize([text] * batch_size).to(device)\n",
    "    part_clip_embedding = clip_model.encode_text(tokenized_text).float()[:, :, None, None]\n",
    "    print(f\"{last_pos}:{pos}={text}\")\n",
    "    clip_embedding[:, :, :, last_pos:pos] = part_clip_embedding\n",
    "    last_pos = pos\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        sampled = sample(model, clip_embedding, T=12, size=latent_shape, starting_t=0, temp_range=[1.0, 1.0],\n",
    "           typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=5, renoise_steps=11,\n",
    "           renoise_mode=\"start\")\n",
    "    sampled = decode(sampled[-1], latent_shape)\n",
    "    \n",
    "showimages(sampled)\n",
    "saveimages(sampled, mode + \"_\" + \":\".join(list(map(lambda x: x[0], conditions))), nrow=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65e885-e166-4997-bf81-94e344e46a78",
   "metadata": {},
   "source": [
    "#### Load Image: Disk or Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc57b1a-de96-4ef4-b699-0417078b9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = preprocess(Image.open(\"path_to_image\")).unsqueeze(0).expand(4, -1, -1, -1).to(device)[:, :3]\n",
    "showimages(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf26db-a58a-4f53-9c0b-d83707e7713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://media.istockphoto.com/id/1193591781/photo/obedient-dog-breed-welsh-corgi-pembroke-sitting-and-smiles-on-a-white-background-not-isolate.jpg?s=612x612&w=0&k=20&c=ZDKTgSFQFG9QvuDziGsnt55kvQoqJtIhrmVRkpYqxtQ=\"\n",
    "# url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1200px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "images = preprocess(img).unsqueeze(0).expand(4, -1, -1, -1).to(device)[:, :3]\n",
    "showimages(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb27d7-5d3b-4a2d-9b2a-86cb81850dad",
   "metadata": {},
   "source": [
    "# Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59c35c-b7a5-4468-99b8-4e4377df63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"inpainting\"\n",
    "text = \"a delicious spanish paella\"\n",
    "tokenized_text = tokenizer.tokenize([text] * images.shape[0]).to(device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        # clip_embeddings = clip_model.encode_image(clip_preprocess(images)).float() # clip_embeddings = clip_model.encode_text(text).float()\n",
    "        clip_embeddings = clip_model.encode_text(tokenized_text).float()\n",
    "        encoded_tokens = encode(images)\n",
    "        latent_shape = encoded_tokens.shape[1:]\n",
    "        mask = torch.zeros_like(encoded_tokens)\n",
    "        mask[:,5:28,5:28] = 1\n",
    "        sampled = sample(model, clip_embeddings, x=encoded_tokens, mask=mask, T=12, size=latent_shape, starting_t=0, temp_range=[1.0, 1.0],\n",
    "               typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=6, renoise_steps=11)\n",
    "    sampled = decode(sampled[-1], latent_shape)\n",
    "\n",
    "showimages(images[0:1], height=10, width=10)\n",
    "showmask(mask[0:1])\n",
    "showimages(sampled, height=16, width=16)\n",
    "saveimages(torch.cat([images[0:1], sampled]), mode + \"_\" + text, nrow=images.shape[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2bd06-aef7-4887-a5e8-dcc4af883dab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Outpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4243a-186e-40e1-9b71-c11df0c60963",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"outpainting\"\n",
    "size = (40, 64)\n",
    "top_left = (0, 16)\n",
    "text = \"black & white photograph of a rocket from the bottom.\"\n",
    "tokenized_text = tokenizer.tokenize([text] * images.shape[0]).to(device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        # clip_embeddings = clip_model.encode_image(clip_preprocess(images)).float()\n",
    "        clip_embeddings = clip_model.encode_text(tokenized_text).float()\n",
    "        encoded_tokens = encode(images)\n",
    "        canvas = torch.zeros((images.shape[0], *size), dtype=torch.long).to(device)\n",
    "        canvas[:, top_left[0]:top_left[0]+encoded_tokens.shape[1], top_left[1]:top_left[1]+encoded_tokens.shape[2]] = encoded_tokens\n",
    "        mask = torch.ones_like(canvas)\n",
    "        mask[:, top_left[0]:top_left[0]+encoded_tokens.shape[1], top_left[1]:top_left[1]+encoded_tokens.shape[2]] = 0\n",
    "        sampled = sample(model, clip_embeddings, x=canvas, mask=mask, T=12, size=size, starting_t=0, temp_range=[1.0, 1.0],\n",
    "               typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=4, renoise_steps=11)\n",
    "    sampled = decode(sampled[-1], size)\n",
    "\n",
    "showimages(images[0:1], height=10, width=10)\n",
    "showmask(mask[0:1])\n",
    "showimages(sampled, height=16, width=16)\n",
    "saveimages(sampled, mode + \"_\" + text, nrow=images.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad2d6da-ae1c-4c76-a494-ceb5469a649b",
   "metadata": {},
   "source": [
    "# Structural Morphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46fd02-6770-4824-ad1d-356c6f197eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"morphing\"\n",
    "max_steps = 24\n",
    "init_step = 8\n",
    "\n",
    "text = \"A fox posing for a photo. stock photo. highly detailed. 4k\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        # images = preprocess(Image.open(\"data/city sketch.png\")).unsqueeze(0).expand(4, -1, -1, -1).to(device)[:, :3]\n",
    "        latent_image = encode(images)\n",
    "        latent_shape = latent_image.shape[-2:]\n",
    "        r = torch.ones(latent_image.size(0), device=device) * (init_step/max_steps)\n",
    "        noised_latent_image, _ = model.add_noise(latent_image, r)\n",
    "        \n",
    "        tokenized_text = tokenizer.tokenize([text] * images.size(0)).to(device)\n",
    "        clip_embeddings = clip_model.encode_text(tokenized_text).float()\n",
    "        \n",
    "        sampled = sample(model, clip_embeddings, x=noised_latent_image, T=max_steps, size=latent_shape, starting_t=init_step, temp_range=[1.0, 1.0],\n",
    "                   typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=6, renoise_steps=max_steps-1,\n",
    "                    renoise_mode=\"prev\")\n",
    "    sampled = decode(sampled[-1], latent_shape)\n",
    "showimages(sampled)\n",
    "showimages(images)\n",
    "saveimages(torch.cat([images[0:1], sampled]), mode + \"_\" + text, nrow=images.shape[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109dbf24-8b68-4af1-9fa2-492bbf5c6a26",
   "metadata": {},
   "source": [
    "# Image Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81732b-9adc-4cda-9961-12480d932ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224), interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72cd16-0f3e-407e-ba5e-ad7664254c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_shape = (32, 32)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        clip_embeddings = clip_model.encode_image(clip_preprocess(images)).float()       # clip_embeddings = clip_model.encode_text(text).float() \n",
    "        sampled = sample(model, clip_embeddings, T=12, size=latent_shape, starting_t=0, temp_range=[1.0, 1.0],\n",
    "               typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=5, renoise_steps=11)\n",
    "    sampled = decode(sampled[-1], latent_shape)\n",
    "\n",
    "showimages(images)\n",
    "showimages(sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446993a-e5b4-40b9-a898-f2b8e9e03576",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Experimental: Concept Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32d746-3cb1-4a27-a537-0e0baad20830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encode(x, clip_model, insertion_index):\n",
    "    # x = x.type(clip_model.dtype)\n",
    "    x = x + clip_model.positional_embedding\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "    x = clip_model.transformer(x)\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "    x = clip_model.ln_final(x)\n",
    "\n",
    "    # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "    # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "    x = x[torch.arange(x.shape[0]), insertion_index] @ clip_model.text_projection\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391e34d-ddab-4718-83d3-3cbf56c19363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "batch_size = 1\n",
    "asteriks_emb = clip_model.token_embedding(tokenizer.tokenize([\"*\"]).to(device))[0][1]\n",
    "context_word = torch.randn(batch_size, 1, asteriks_emb.shape[-1]).to(device)\n",
    "context_word.requires_grad_(True)\n",
    "optim = AdamW(params=[context_word], lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c9101-c62e-465c-a3aa-2d2afd5de075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "_preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(256),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "urls = [\n",
    "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcStVHtFcMqIP4xuDYn8n_FzPDKjPtP_iTSbOQ&usqp=CAU\",\n",
    "    \"https://i.insider.com/58d919eaf2d0331b008b4bbd?width=700\",\n",
    "    \"https://media.cntraveler.com/photos/5539216cab60aad20f3f3aaa/16:9/w_2560%2Cc_limit/eiffel-tower-paris-secret-apartment.jpg\",\n",
    "    \"https://static.independent.co.uk/s3fs-public/thumbnails/image/2014/03/25/12/eiffel.jpg?width=1200\"\n",
    "]\n",
    "images = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    images.append(_preprocess(img))\n",
    "\n",
    "data = torch.stack(images)\n",
    "dataset = DataLoader(TensorDataset(data), batch_size=1, shuffle=True)\n",
    "loader = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db5f60-41d5-47e5-9c4b-b9b2911467e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps = 100\n",
    "total_loss = 0\n",
    "total_acc = 0\n",
    "pbar = tqdm(range(steps))\n",
    "for i in pbar:\n",
    "    try:\n",
    "        images = next(loader)[0]\n",
    "    except StopIteration:\n",
    "        loader = iter(dataset)\n",
    "        images = next(loader)[0]\n",
    "    images = images.to(device)\n",
    "    text = \"a photo of *\"\n",
    "    tokenized_text = tokenizer.tokenize([text]).to(device)\n",
    "    insertion_index = tokenized_text.argmax(dim=-1)\n",
    "    neutral_text_encoded = clip_model.token_embedding(tokenized_text)\n",
    "    insertion_idx = torch.where(neutral_text_encoded == asteriks_emb)[1].unique()\n",
    "    neutral_text_encoded[:, insertion_idx, :] = context_word\n",
    "    clip_embeddings = text_encode(neutral_text_encoded, clip_model, insertion_index)\n",
    "    with torch.no_grad():\n",
    "        image_indices = encode(images)\n",
    "        r = torch.rand(images.size(0), device=device)\n",
    "        noised_indices, mask = model.add_noise(image_indices, r)\n",
    "\n",
    "    # with torch.autocast(device_type=\"cuda\"):\n",
    "    pred = model(noised_indices, clip_embeddings, r)\n",
    "    loss = criterion(pred, image_indices)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    acc = (pred.argmax(1) == image_indices).float()  # .mean()\n",
    "    acc = acc.mean()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_acc += acc.item()\n",
    "    pbar.set_postfix({\"total_loss\": total_loss / (i+1), \"total_acc\": total_acc / (i+1)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95a42f-e121-43da-b432-b5f81c408ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        sampled = sample(model, clip_embeddings.expand(4, -1), T=12, size=(32, 32), starting_t=0, temp_range=[1., 1.],\n",
    "               typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=4, renoise_steps=11)\n",
    "    sampled = decode(sampled[-1])\n",
    "\n",
    "plt.figure(figsize=(32, 32))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(torch.cat([\n",
    "    torch.cat([i for i in images.expand(4, -1, -1, -1).cpu()], dim=-1),\n",
    "    torch.cat([i for i in sampled.cpu()], dim=-1),\n",
    "], dim=-2).permute(1, 2, 0).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eac484-b3bf-428a-8e59-1742dbca6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"* at night\"\n",
    "tokenized_text = tokenizer.tokenize([text]).to(device)\n",
    "insertion_index = tokenized_text.argmax(dim=-1)\n",
    "neutral_text_encoded = clip_model.token_embedding(tokenized_text)\n",
    "insertion_idx = torch.where(neutral_text_encoded == asteriks_emb)[1].unique()\n",
    "neutral_text_encoded[:, insertion_idx, :] = context_word\n",
    "clip_embeddings = text_encode(neutral_text_encoded, clip_model, insertion_index)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        sampled = sample(model, clip_embeddings.expand(4, -1), T=12, size=(32, 32), starting_t=0, temp_range=[1., 1.],\n",
    "               typical_filtering=True, typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=4, renoise_steps=11)\n",
    "    sampled = decode(sampled[-1])\n",
    "\n",
    "plt.figure(figsize=(32, 32))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(torch.cat([\n",
    "    torch.cat([i for i in images.expand(4, -1, -1, -1).cpu()], dim=-1),\n",
    "    torch.cat([i for i in sampled.cpu()], dim=-1),\n",
    "], dim=-2).permute(1, 2, 0).cpu())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DenoiseGIT_sampling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
